Open-ended human learning and information-seeking are increasingly mediated by digital assistants.
However, such systems often ignore the user's pre-existing knowledge.
Assuming a correlation between engagement and user responses such as ``liking'' messages or asking followup questions, we design a Wizard-of-Oz dialog task that tests the hypothesis that engagement increases when users are presented with facts related to what they know.
Through crowd-sourcing of this experiment, we collect and release 14K dialogs (181K utterances) where users and assistants converse about geographic topics like geopolitical entities and locations.
This dataset is annotated with pre-existing user knowledge, message-level dialog acts, grounding to Wikipedia, and user reactions to messages.
Responses using a user's prior knowledge increase engagement.
We incorporate this knowledge into a multi-task model that reproduces human assistant policies and improves over a \bert{} content model by 13 mean reciprocal rank points.
